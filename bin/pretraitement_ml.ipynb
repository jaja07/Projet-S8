{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74789c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75c9fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretraitement_machine():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    import datetime\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "    pathfile='data_anonymous'\n",
    "\n",
    "    # reflist: list of epc in each box\n",
    "    reflist=pd.DataFrame()\n",
    "    # \n",
    "    files=os.listdir(pathfile) # retourne une liste des fichiers dans le répertoire pathfile\n",
    "    for file in files:\n",
    "        #print(file)\n",
    "        if file.startswith('reflist_'):\n",
    "            temp=pd.read_csv(os.path.join(pathfile,file),sep=',').reset_index(drop=True)[['Epc']]\n",
    "            temp['refListId']=file.split('.')[0]\n",
    "            reflist = pd.concat([reflist, temp])\n",
    "    reflist=reflist.rename(columns={'refListId':'refListId_actual'})\n",
    "    reflist['refListId_actual']=reflist['refListId_actual'].apply(lambda x:int(x[8:]))\n",
    "    Q_refListId_actual=reflist.groupby('refListId_actual')['Epc'].nunique().rename('Q refListId_actual').reset_index(drop=False)\n",
    "    reflist=pd.merge(reflist,Q_refListId_actual,on='refListId_actual',how='left')\n",
    "    \n",
    "    # 2eme partie\n",
    "    \n",
    "    # pathfile=r'data_anonymous'\n",
    "    # \n",
    "    # df : rfid readings\n",
    "    tags=pd.DataFrame()\n",
    "    # \n",
    "    files=os.listdir(pathfile)\n",
    "    for file in files:\n",
    "        #print(file)\n",
    "        if file.startswith('ano_APTags'):\n",
    "            temp=pd.read_csv(os.path.join(pathfile,file),sep=',')\n",
    "            tags=pd.concat([tags,temp])\n",
    "    tags['LogTime']=pd.to_datetime (tags['LogTime'] ,format='%Y-%m-%d-%H:%M:%S') \n",
    "    tags['TimeStamp']=tags['TimeStamp'].astype(float)\n",
    "    tags['Rssi']=tags['Rssi'].astype(float)\n",
    "    tags=tags.drop(['Reader','EmitPower','Frequency'],axis=1).reset_index(drop=True)\n",
    "    tags=tags[['LogTime', 'Epc', 'Rssi', 'Ant']]\n",
    "    # antennas 1 and 2 are facing the box when photocell in/out \n",
    "    Ant_loc=pd.DataFrame({'Ant':[1,2,3,4],'loc':['in','in','out','out']})\n",
    "    tags=pd.merge(tags,Ant_loc,on=['Ant'])\n",
    "    tags=tags.sort_values('LogTime').reset_index(drop=True)\n",
    "    \n",
    "    # 3eme partie\n",
    "    \n",
    "        # timing: photocells a time window for each box: start/stop (ciuchStart, ciuchStop)\n",
    "    file='ano_supply-process.2019-11-07-CUT.csv'\n",
    "    timing=pd.read_csv(os.path.join(pathfile,file),sep=',')\n",
    "    timing['file']=file\n",
    "    timing['date']=pd.to_datetime(timing['date'],format='%d/%m/%Y %H:%M:%S,%f')\n",
    "    timing['Start']=pd.to_datetime(timing['ciuchStart'],format='%d/%m/%Y %H:%M:%S,%f')\n",
    "    timing['Stop']=pd.to_datetime(timing['ciuchStop'],format='%d/%m/%Y %H:%M:%S,%f')\n",
    "    timing['timestampStart']=timing['timestampStart'].astype(float)\n",
    "    timing['timestampStop']=timing['timestampStop'].astype(float)\n",
    "    timing=timing.sort_values('date')\n",
    "    timing.loc[:,'refListId']=timing.loc[:,'refListId'].apply(lambda x:int(x[8:]))\n",
    "    timing=timing[['refListId', 'Start', 'Stop']]\n",
    "    \n",
    "    # 4eme partie\n",
    "    \n",
    "    # Start_up starts upstream Start, half way in between the previous stop and the actual start\n",
    "    timing[['Stop_last']]=timing[['Stop']].shift(1)\n",
    "    timing[['refListId_last']]=timing[['refListId']].shift(1)\n",
    "    timing['Startup']=timing['Start'] - (timing['Start'] - timing['Stop_last'])/2\n",
    "    # timing start: 10sec before timing\n",
    "    timing.loc[0,'refListId_last']=timing.loc[0,'refListId']\n",
    "    timing.loc[0,'Startup']=timing.loc[0,'Start']-datetime.timedelta(seconds=10)\n",
    "    timing.loc[0,'Stop_last']=timing.loc[0,'Startup']-datetime.timedelta(seconds=10)\n",
    "    timing['refListId_last']=timing['refListId_last'].astype(int)\n",
    "    # \n",
    "    timing['Stopdown']= timing['Startup'].shift(-1)\n",
    "    timing.loc[len(timing)-1,'Stopdown']=timing.loc[len(timing)-1,'Stop']+datetime.timedelta(seconds=10)\n",
    "    timing=timing[['refListId', 'refListId_last','Startup', 'Start','Stop','Stopdown']]\n",
    "    #timing.head(30)\n",
    "    \n",
    "    # 5eme partie\n",
    "    \n",
    "    # t0_run = a new run starts when box 0 shows up\n",
    "    # t0_run: c'est l'instant à partir duquel commence un run, chaque run commence avec la boxe 0\n",
    "    # run : id du run\n",
    "    t0_run=timing[timing['refListId']==0] [['Startup']]\n",
    "    t0_run=t0_run.rename(columns={'Startup':'t0_run'})\n",
    "    t0_run=t0_run.groupby('t0_run').size().cumsum().rename('run').reset_index(drop=False)\n",
    "    t0_run=t0_run.sort_values('t0_run')\n",
    "    # \n",
    "    # each row in timing is merged with a last row in t0_run where t0_run (ciuchstart) <= timing (ciuchstart)\n",
    "    timing=pd.merge_asof(timing,t0_run,left_on='Startup',right_on='t0_run', direction='backward')\n",
    "    timing=timing.sort_values('Stop')\n",
    "    timing=timing[['run', 'refListId', 'refListId_last', 'Startup','Start','Stop','Stopdown','t0_run']]\n",
    "    timing.head(20)\n",
    "    #timing['run'].value_counts()\n",
    "    \n",
    "    # 6eme partie\n",
    "    \n",
    "        #  full window (Startup > Stopdown) is sliced in smaller slices\n",
    "    # Startup > Start: 11 slices named up_0, up_1, ..., up_10\n",
    "    # Start > Stop: 11 slices named mid_0, mid_1, ... mid_10\n",
    "    # Stop > Stopdown: 11 slices names down_0, down_1, ... down_10\n",
    "    slices=pd.DataFrame()\n",
    "    for i, row in timing.iterrows(): \n",
    "        ciuchStartup=row['Startup']\n",
    "        ciuchStart=row['Start']\n",
    "        ciuchStop=row['Stop']\n",
    "        ciuchStopdown=row['Stopdown']\n",
    "        steps=4\n",
    "    #     \n",
    "        up=pd.DataFrame(index=pd.date_range(start=ciuchStartup, end=ciuchStart,periods=steps)).reset_index(drop=False).rename(columns={'index':'slice'})\n",
    "        up.index=['up_'+str(x) for x in range(steps)]\n",
    "        slices= pd.concat([slices, up])    \n",
    "  \n",
    "        mid=pd.DataFrame(index=pd.date_range(start=ciuchStart, end=ciuchStop,periods=steps)).reset_index(drop=False).rename(columns={'index':'slice'})\n",
    "        mid.index=['mid_'+str(x) for x in range(steps)]\n",
    "        slices= pd.concat([slices, mid])\n",
    "#     \n",
    "        down=pd.DataFrame(index=pd.date_range(start=ciuchStop, end=ciuchStopdown,periods=steps)).reset_index(drop=False).rename(columns={'index':'slice'})\n",
    "        down.index=['down_'+str(x) for x in range(steps)]\n",
    "        slices= pd.concat([slices, down])\n",
    "\n",
    "    slices=slices.reset_index(drop=False).rename(columns={'index':'slice_id'})\n",
    "    # \n",
    "    timing_slices=pd.merge_asof(slices,timing,left_on='slice',right_on='Startup',direction='backward')\n",
    "    timing_slices=timing_slices[['run', 'refListId', 'refListId_last','slice_id','slice',  \\\n",
    "                             'Startup', 'Start', 'Stop', 'Stopdown','t0_run']]\n",
    "    \n",
    "    # 7eme partie\n",
    "    \n",
    "    # merge between tags and timing\n",
    "    # merge_asof needs sorted df > df_ref\n",
    "    df = tags\n",
    "    df=df[(tags['LogTime']>=timing['Startup'].min()) & (df['LogTime']<=timing['Stopdown'].max())]\n",
    "    df=df.sort_values('LogTime')\n",
    "    df_timing_slices=pd.merge_asof(df,timing_slices,left_on=['LogTime'],right_on=['slice'],direction='backward')\n",
    "    df_timing_slices=df_timing_slices.dropna()\n",
    "    df_timing_slices=df_timing_slices.sort_values('slice').reset_index(drop=True)\n",
    "    df_timing_slices['window_run_id']  = df_timing_slices['refListId'].astype(str) +'_'+ df_timing_slices['run'].astype(str)\n",
    "    df_timing_slices=df_timing_slices[['run', 'window_run_id', 'Epc','refListId', 'refListId_last', 'Startup','slice_id','slice','LogTime','Start','Stop', 'Stopdown', 'Rssi', 'loc','t0_run']]\n",
    "    df_timing_slices\n",
    "    \n",
    "    # 9eme partie\n",
    "    \n",
    "    runs_out=df_timing_slices .groupby('run')['refListId'].nunique().rename('Q refListId').reset_index(drop=False)\n",
    "    runs_out[runs_out['Q refListId']!=10]\n",
    "    \n",
    "    # 10eme partie\n",
    "    \n",
    "    current_last_windows=timing_slices.drop_duplicates(['run','refListId','refListId_last'])\n",
    "    current_last_windows=current_last_windows[['run','refListId','refListId_last','Stop']].reset_index(drop=True)\n",
    "    current_last_windows[:1]\n",
    "    \n",
    "    #11eme partie\n",
    "    \n",
    "    # runs 16 23 32 40 have missing boxes: discarded\n",
    "    # also run 1 is the start, no previous box: discarded\n",
    "    # run 18: box 0 run at the end\n",
    "    # \n",
    "    timing=timing[~timing['run'].isin([1,18,16,23,32,40])]\n",
    "    timing_slices=timing_slices[~timing_slices['run'].isin([1,18,16,23,32,40])]\n",
    "    df_timing_slices=df_timing_slices[~df_timing_slices['run'].isin([1,18,16,23,32,40])]\n",
    "    \n",
    "    #12eme partie\n",
    "    df_timing_slices=df_timing_slices.sort_values(['LogTime','Epc'])\n",
    "    #\n",
    "    #13eme partie\n",
    "    len(timing),len(timing_slices), len(df_timing_slices)\n",
    "    \n",
    "    #14eme partie\n",
    "    \n",
    "    # df_timing_slices['dt']=\n",
    "    df_timing_slices['dt']=(df_timing_slices['LogTime']-df_timing_slices['t0_run']).apply(lambda x:x.total_seconds())\n",
    "    \n",
    "    #15eme partie\n",
    "    rssi_threshold=-110\n",
    "    df_timing_slices_threshold=df_timing_slices[df_timing_slices['Rssi']>rssi_threshold]\n",
    "    timing['window_width'] = (timing['Stopdown'] - timing['Startup']).apply(lambda x:x.total_seconds())\n",
    "    timing['window_run_id'] = timing['refListId'].astype(str) +\"_\"+ timing['run'].astype(str)\n",
    "    \n",
    "    Features=pd.DataFrame(\n",
    "        [\n",
    "            ['all', True, True, False, True, True, True ],\n",
    "            ['rssi & rc only', True, True, False, False, False, False],\n",
    "            ['rssi & rc_mid', True, True, True, False, False, False ],\n",
    "            ['rssi only', True, False, True, False, False, False ],\n",
    "            ['rc only', False, True, False, False, False, False ],\n",
    "        ],\n",
    "        columns=['features', 'rssi', 'rc', 'rc_mid_only', 'Epcs_window', 'reads_window', 'window_width']\n",
    "    ) \n",
    "    #return df_timing_slices, timing, reflist\n",
    "\n",
    "\n",
    "\n",
    "#def dataset ()\n",
    "    \n",
    "    #df_timing_slices, timing, reflist = pretraitement_machine()\n",
    "    # regroupe le dataset par les colonnes suivantes 'Epc', 'window_run_id', 'slice_id', 'loc' puis calule le quantile mediant c'est à dire divise le dataset en deux parties égales\n",
    "    ds_rssi = df_timing_slices.groupby(['Epc', 'window_run_id', 'slice_id', 'loc'])['Rssi'].quantile(1)\\\n",
    "        .unstack(['slice_id', 'loc'], fill_value=-110)\n",
    "    ds_rssi.columns = [x[0]+'_'+x[1] for x in ds_rssi.columns]\n",
    "    ds_rssi = ds_rssi.reset_index(drop = False)\n",
    "    \n",
    "    ds_rc = df_timing_slices.groupby(['Epc', 'window_run_id', 'slice_id', 'loc']).size()\\\n",
    "        .unstack(['slice_id', 'loc'], fill_value=0)\n",
    "    ds_rc.columns = [x[0]+'_'+x[1] for x in ds_rc.columns]\n",
    "    ds_rc = ds_rc.reset_index(drop = False)\n",
    "    \n",
    "    ds = pd.merge(ds_rssi, ds_rc, on=['Epc', 'window_run_id'], suffixes=['_rssi', '_rc'])\n",
    "    \n",
    "    #window_width\n",
    "    \n",
    "    ds = pd.merge(ds, timing[['window_run_id', 'window_width']], on ='window_run_id', how='left')\n",
    "    \n",
    "        #Epcs_window\n",
    "    \n",
    "    Q_Epcs_window = df_timing_slices.groupby(['window_run_id'])['Epc'].nunique().rename('Epcs_window').reset_index(drop=False)\n",
    "    ds= pd.merge(ds, Q_Epcs_window, on='window_run_id', how='left')\n",
    "    \n",
    "    #reads_window\n",
    "    \n",
    "    Q_reads_window = df_timing_slices.groupby(['window_run_id']).size().rename('reads_window').reset_index(drop=False)\n",
    "    ds= pd.merge(ds, Q_reads_window, on= 'window_run_id', how='left')\n",
    "    ds = pd.merge(ds, reflist, on='Epc', how='left')\n",
    "    \n",
    "    ds['actual']= ds['window_run_id'].apply(lambda x:x.split('_')[0]).astype('int64') == ds['refListId_actual']\n",
    "    \n",
    "    ds['actual'] = ds.apply(lambda row: 'in' if int(row['window_run_id'].split('_')[0]) == row['refListId_actual'] else 'out', axis=1)\n",
    "    vrai=ds[ds['actual']== 'in']\n",
    "    faux=ds[ds['actual']== 'out']\n",
    "    true= vrai.shape[0]\n",
    "    false= faux.shape[0] \n",
    "    #return ds\n",
    "\n",
    "#def Xcols_func(features, dataset, rssi_quantile):\n",
    "    \n",
    "    \n",
    "    #df_timing_slices, timing, reflist = pretraitement_machine()\n",
    "    #df = dataset(pretraitement_machine,rssi_quantile)\n",
    "    #Xcols_all = dataset(pretraitement_machine,rssi_quantile).columns\n",
    "    Features_temp = Features [Features['features'] == 'all']\n",
    "    \n",
    "    X=[]\n",
    "    rssi = Features_temp ['rssi'].values[0]\n",
    "    rc = Features_temp['rc'].values[0]\n",
    "    rc_mid_only = Features_temp['rc_mid_only'].values[0]\n",
    "    Epcs_window = Features_temp['Epcs_window'].values[0]\n",
    "    reads_window = Features_temp['reads_window'].values[0]\n",
    "    window_width = Features_temp['window_width'].values[0]\n",
    "    \n",
    "    X_rssi = [x for x in ds.columns if rssi*'rssi' in x.split('_') ]\n",
    "    \n",
    "    X_rc = [x for x in ds.columns if rc*'rc' in x.split('_') ]\n",
    "    \n",
    "    X = X_rssi + X_rc\n",
    "    \n",
    "    if Epcs_window:\n",
    "        X.append('Epcs_window')\n",
    "    if Epcs_window:\n",
    "        X.append('reads_window')\n",
    "    if Epcs_window:\n",
    "        X.append('window_width')\n",
    "    \n",
    "    column_names = X\n",
    "\n",
    "\n",
    "    filtered_df = pd.DataFrame()  # Créer un nouveau DataFrame pour stocker les résultats filtrés\n",
    "    for column in column_names:\n",
    "        if column in df.columns:\n",
    "            # Si le nom de la colonne est présent dans le DataFrame d'origine\n",
    "            # Ajouter la colonne correspondante dans le DataFrame filtré\n",
    "            filtered_df[column] = df[column]\n",
    "    return filtered_df,df['actual']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f888c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
